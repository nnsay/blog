---
title: ä½¿ç”¨ AWS SageMaker å¾®è°ƒæ¨¡å‹
date: 2025-08-15 17:02:36
tags:
  - DevOps
  - AWS
  - AI
excerpt: å­¦ä¹ å¤§æ¨¡å‹å¾®è°ƒå„ä¸ªæ­¥éª¤
---

# å£°æ˜

æœ¬æ–‡ç”± ğŸ¤–AI æ”¹å†™æ¶¦è‰²è€Œæˆ

# **å¼•è¨€ï¼šä¸ºä½•æˆ‘ä»¬éœ€è¦å¾®è°ƒå¤§æ¨¡å‹ï¼Ÿ**

é€šç”¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆå¦‚ GPTã€Llamaï¼‰å·²å…·å¤‡æƒŠäººçš„çŸ¥è¯†å¹¿åº¦å’Œè¯­è¨€èƒ½åŠ›ï¼Œä½†å½“é¢ä¸´é«˜åº¦å‚ç›´æˆ–ä¸“ä¸šçš„é¢†åŸŸæ—¶ï¼Œå®ƒä»¬å¯èƒ½ä¼šæ˜¾å¾—â€œåšè€Œä¸ç²¾â€ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å¾®è°ƒï¼ˆFine-Tuningï¼‰æŠ€æœ¯åº”è¿è€Œç”Ÿï¼Œå®ƒé€šè¿‡åœ¨ç‰¹å®šçš„å°è§„æ¨¡æ•°æ®é›†ä¸Šå¯¹é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œâ€œäºŒæ¬¡è®­ç»ƒâ€ï¼Œå°†ä¸€ä¸ªâ€œé€šæ‰â€æ¨¡å‹é›•ç¢æˆç‰¹å®šé¢†åŸŸçš„â€œä¸“å®¶â€ã€‚

æœ¬æ•™ç¨‹å°†ä»¥ä¸€ä¸ªçœŸå®åœºæ™¯ä¸ºä¾‹ï¼š**å°† Meta çš„ Llama 3.2 1B æŒ‡ä»¤æ¨¡å‹å¾®è°ƒä¸ºä¸€ä¸ªå…‰ä¼è¡Œä¸šæ”¿ç­–è§£è¯»ä¸“å®¶**ã€‚æˆ‘ä»¬å°†èµ°é€šä»æ•°æ®å‡†å¤‡åˆ°æ¨¡å‹éƒ¨ç½²çš„å®Œæ•´é“¾è·¯ï¼Œä¸ºæ‚¨æä¾›ä¸€å¥—å¯å¤ç°ã€å¯æ‰©å±•çš„å®æˆ˜å·¥ä½œæµã€‚

**é¡¹ç›®æµç¨‹æ¦‚è§ˆ:**
`[æ•°æ®çˆ¬å–] -> [å¯¹è¯ç”Ÿæˆ] -> [æ ¼å¼è½¬æ¢] -> [ä¸Šä¼ S3] -> [SageMakerå¾®è°ƒ] -> [SageMakeréƒ¨ç½²] -> [APIè°ƒç”¨]`

# **1. æ ¸å¿ƒæ¦‚å¿µï¼šå¤§æ¨¡å‹å¾®è°ƒå…¥é—¨**

åœ¨å¼€å§‹å®æˆ˜ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆå¿«é€Ÿäº†è§£å‡ ä¸ªæ ¸å¿ƒæ¦‚å¿µã€‚

- **ä»€ä¹ˆæ˜¯å¤§æ¨¡å‹å¾®è°ƒï¼Ÿ**

  ç®€å•æ¥è¯´ï¼Œå¾®è°ƒå°±æ˜¯åœ¨é¢„è®­ç»ƒå¥½çš„é€šç”¨å¤§æ¨¡å‹åŸºç¡€ä¸Šï¼Œä½¿ç”¨æˆ‘ä»¬è‡ªå·±å‡†å¤‡çš„ã€é’ˆå¯¹ç‰¹å®šä»»åŠ¡çš„æ•°æ®é›†ï¼Œå¯¹æ¨¡å‹è¿›è¡Œå°èŒƒå›´çš„å‚æ•°æ›´æ–°ã€‚è¿™ä¸ªè¿‡ç¨‹å¼¥åˆäº†é€šç”¨çŸ¥è¯†ä¸ç‰¹å®šéœ€æ±‚ä¹‹é—´çš„é¸¿æ²Ÿï¼Œè®©æ¨¡å‹èƒ½å¤Ÿæ›´ç²¾å‡†åœ°ç†è§£ä¸“ä¸šæœ¯è¯­ã€éµå¾ªç‰¹å®šæŒ‡ä»¤ã€æˆ–æ¨¡ä»¿æŸç§è¯´è¯é£æ ¼ã€‚

  ä¾‹å¦‚ï¼Œä¸€ä¸ªé€šç”¨çš„èŠå¤©æ¨¡å‹å¯èƒ½æ— æ³•å‡†ç¡®å›ç­”â€œ2025 å¹´åˆ†å¸ƒå¼å…‰ä¼å¹¶ç½‘è¡¥è´´çš„å…·ä½“æ”¿ç­–ä¾æ®æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼Œä½†ç»è¿‡å…‰ä¼æ”¿ç­–æ•°æ®å¾®è°ƒåï¼Œå®ƒå°±èƒ½ç»™å‡ºä¸“ä¸šä¸”ç²¾å‡†çš„å›ç­”ã€‚

- **ä¸»æµå¾®è°ƒæ–¹æ³•ç®€ä»‹**

  - **æŒç»­é¢„è®­ç»ƒ (Continual Pre-Training, CPT):** ä½¿ç”¨å¤§é‡æ— ç›‘ç£çš„è¡Œä¸šè¯­æ–™ï¼ˆå¦‚æµ·é‡è¡Œä¸šæ–‡æ¡£ï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¹ ç‰¹å®šé¢†åŸŸçš„è¯­è¨€èŒƒå¼å’ŒèƒŒæ™¯çŸ¥è¯†ã€‚æ­¤æ–¹æ³•èƒ½æå‡æ¨¡å‹çš„è¡Œä¸šè¡¨ç°ï¼Œä½†éœ€è¦å·¨å¤§çš„æ•°æ®é‡ï¼ˆé€šå¸¸åƒä¸‡çº§ Token ä»¥ä¸Šï¼‰ã€‚
  - **ç›‘ç£å¼å¾®è°ƒ (Supervised Fine-Tuning, SFT):** ä½¿ç”¨é«˜è´¨é‡çš„â€œæŒ‡ä»¤-å›ç­”â€æˆ–å¯¹è¯å¼æ•°æ®é›†ï¼Œæ•™æ¨¡å‹å¦‚ä½•å“åº”ç‰¹å®šç±»å‹çš„æé—®ã€‚æ­¤æ–¹æ³•èƒ½é«˜æ•ˆæå‡æ¨¡å‹åœ¨ç‰¹å®šä¸šåŠ¡åœºæ™¯ä¸‹çš„è¡¨ç°ï¼Œæ‰€éœ€æ•°æ®é‡ç›¸å¯¹è¾ƒå°ï¼ˆæ•°ç™¾è‡³æ•°åƒæ¡å³å¯ï¼‰ã€‚
  - **ç›´æ¥åå¥½ä¼˜åŒ– (Direct Preference Optimization, DPO):** é€šè¿‡â€œä¼˜é€‰å›ç­”â€å’Œâ€œåŠ£è´¨å›ç­”â€çš„å¯¹æ¯”æ•°æ®ï¼Œè®©æ¨¡å‹å­¦ä¹ äººç±»çš„åå¥½ï¼Œè¿›ä¸€æ­¥æå‡å›ç­”çš„è´¨é‡å’Œå®‰å…¨æ€§ã€‚

  è€ƒè™‘åˆ°æ•°æ®è·å–æˆæœ¬å’Œä»»åŠ¡ç›®æ ‡ï¼Œ**æœ¬æ–‡å°†é‡‡ç”¨ SFT æ–¹æ³•**ï¼Œå®ƒæ˜¯åœ¨æ€§èƒ½å’Œèµ„æºæŠ•å…¥ä¹‹é—´å–å¾—å¹³è¡¡çš„æœ€ä½³é€‰æ‹©ã€‚

# **2. å‡†å¤‡æ•°æ®ï¼šä»åŸå§‹æ–°é—»åˆ°å¯¹è¯è¯­æ–™**

é«˜è´¨é‡çš„æ•°æ®æ˜¯å¾®è°ƒæˆåŠŸçš„åŸºçŸ³ã€‚æˆ‘ä»¬å°†åˆ†æ­¥å®Œæˆä»åŸå§‹æ•°æ®é‡‡é›†åˆ°åˆå§‹å¯¹è¯æ•°æ®é›†ç”Ÿæˆçš„å…¨è¿‡ç¨‹ã€‚

- **å·¥ä½œæµç¨‹ï¼š**

  1.  **æ•°æ®é‡‡é›†ï¼š** ä½¿ç”¨çˆ¬è™«å·¥å…· `crawl4ai` ä»â€œåŒ—ææ˜Ÿå…‰ä¼ç½‘â€æŠ“å–è¿‘æœŸçš„æ”¿ç­–æ–°é—» URLã€‚
  2.  **å†…å®¹æå–ï¼š** éå† URL åˆ—è¡¨ï¼ŒæŠ“å–æ¯ç¯‡æ–°é—»çš„è¯¦ç»†å†…å®¹ï¼Œå¹¶ä¿å­˜ä¸º Markdown æ ¼å¼ã€‚
  3.  **å¯¹è¯ç”Ÿæˆï¼š** **è¿™æ˜¯å…³é”®ä¸€æ­¥**ã€‚æˆ‘ä»¬å·§å¦™åœ°åˆ©ç”¨ç¬¬ä¸‰æ–¹å¼ºåŠ›å¤§æ¨¡å‹ï¼ˆå¦‚ GPT-4oï¼‰ä½œä¸ºâ€œæ•°æ®æ ‡æ³¨ä¸“å®¶â€ï¼Œä¸ºæ¯ä¸€ç¯‡æ–°é—»æ–‡ç« è‡ªåŠ¨ç”Ÿæˆé«˜è´¨é‡ã€å¤šè§’åº¦çš„é—®ç­”å¯¹è¯ã€‚
  4.  **æ ¼å¼åŒ–å­˜å‚¨ï¼š** å°†æ‰€æœ‰ç”Ÿæˆçš„å¯¹è¯æ•°æ®å­˜å‚¨ä¸ºç»“æ„åŒ–çš„ `JSONL` æ–‡ä»¶ï¼Œä¸ºä¸‹ä¸€æ­¥å¤„ç†åšå¥½å‡†å¤‡ã€‚

- **ä»£ç å®ç°ï¼š**

- main.py

```python
import asyncio
import glob
import os
from pathlib import Path
from typing import Dict, Tuple
import pandas as pd
from datasets import Dataset
from src.llm import generate_conversation
from src.crawl import get_policy, get_policy_urls


async def main():
    policy_csv_file = "output/policy_urls.csv"
    policy_urls = await get_policy_urls()
    policy_df = pd.DataFrame(policy_urls)
    if os.path.exists(policy_csv_file):
        existing_df = pd.read_csv(policy_csv_file)
        policy_df = pd.concat([existing_df, policy_df], ignore_index=True)
        policy_df.drop_duplicates(subset=["link"], keep="last", inplace=True)
    policy_df.set_index("date", inplace=True)
    policy_df = policy_df.sort_index(ascending=False)
    policy_df.to_csv(policy_csv_file)

    total_count = len(policy_urls)
    finished_count = 0
    for url in policy_urls:
        policy_conversation_csv = f"output/conversations/{url.file_id}.csv"
        if os.path.exists(policy_conversation_csv):
            finished_count += 1
            print(
                f"policy <{url.title}> conversation generation done(1) ({finished_count}/{total_count})"
            )
            continue

        policy_news_md = f"output/policy-news/{url.file_id}.md"
        if os.path.exists(policy_conversation_csv):
            with open(policy_news_md, "r", encoding="utf-8") as md:
                content = md.read()
        else:
            content = await get_policy(url.link)
            with open(policy_news_md, "w", encoding="utf-8") as md:
                md.write(content)

        conversations = generate_conversation(content)
        finished_count += 1
        print(
            f"policy <{url.title}> conversation generation done(2) ({finished_count}/{total_count})"
        )
        df = pd.DataFrame(conversations)
        df.set_index("role", inplace=True)
        df.to_csv(policy_conversation_csv)

    generate_dataset()


def generate_dataset():
    policy_info: Dict[str, Tuple[str, str]] = {}
    df = pd.read_csv("output/policy_urls.csv")
    for _, row in df.iterrows():
        policy_info[str(row["file_id"])] = (row["date"], row["title"])

    all_csv_files = glob.glob(os.path.join("output/conversations", "*.csv"))
    filed_name = "conversations"
    all_messages = []
    for csv_file in all_csv_files:
        file_id = Path(csv_file).stem
        _, title = policy_info[file_id]
        df = pd.read_csv(csv_file)
        messages = []
        for _, row in df.iterrows():
            role, content = row["role"], row["content"]
            if role == "user":
                content = f"å…³äºå…‰ä¼æ”¿ç­–<{title}>, ç”¨æˆ·æé—®: {content}"
            messages.append({"role": role, "content": content})
        all_messages.append({filed_name: messages})

    pretty_df = pd.DataFrame(all_messages)
    # generate jsonl format dataset
    pretty_df.to_json(
        "output/dataset/special.jsonl",
        orient="records",
        lines=True,
        force_ascii=False,
    )
    print("generate jsonl format dataset done")

    # generate arrow format dataset
    ds = Dataset.from_pandas(pretty_df)
    ds.save_to_disk("output/dataset/special")
    print("generate arrow format dataset done")


if __name__ == "__main__":
    asyncio.run(main())
```

`````

- src/crawl.py

```python
import json
from typing import List
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CacheMode,
    CrawlerRunConfig,
    JsonCssExtractionStrategy,
)
from urllib.parse import urlparse
from pathlib import Path


class PolicyUrl(dict):
    title: str
    link: str
    date: str
    file_id: str

    def __init__(self, title, link, date, file_id):
        super().__init__(title=title, link=link, date=date, file_id=file_id)
        self.title = title
        self.link = link
        self.date = date
        self.file_id = file_id


async def get_policy_urls(max_pages: int = 6):
    policy_url_schema = {
        "name": "policy news",
        "baseSelector": "div.cc-layout-3 .box .center li",
        "fields": [
            {
                "name": "title",
                "selector": "a",
                "type": "text",
            },
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"},
            {"name": "date", "selector": "span", "type": "text"},
        ],
    }
    urls = [f"https://guangfu.bjx.com.cn/zc/{page}" for page in range(1, max_pages + 1)]
    browser_conf = BrowserConfig(headless=True, java_script_enabled=True)
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(policy_url_schema),
        wait_for="css:.cc-layout-3 .box .center",
    )
    result: List[PolicyUrl] = []
    async with AsyncWebCrawler(config=browser_conf) as crawler:  # type: ignore
        all_res = await crawler.arun_many(urls, config=run_conf)
        for res in all_res:  # type: ignore
            if not res.success:
                print(f"[ERROR] {res.url} => {res.error_message}")
                continue
            policyUrls = json.loads(res.extracted_content)
            for url in policyUrls:
                path_str = urlparse(url["link"]).path
                file_id = Path(path_str).stem
                result.append(PolicyUrl(**url, file_id=file_id))
    return result


async def get_policy(policy_url: str) -> str:
    browser_conf = BrowserConfig(headless=True, java_script_enabled=True)
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        css_selector="div#article_cont",
        wait_for="css:.cc-article",
    )
    async with AsyncWebCrawler(config=browser_conf) as crawler:
        result = await crawler.arun(url=policy_url, config=run_conf)
        return result.markdown  # type: ignore
```

- src/llm.py

````python
import json
import os
import re
from typing import List
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv(override=True)


def build_prompt(markdown_text: str) -> str:
    return f"""
# è§’è‰²

ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„AIæ•°æ®æ ‡æ³¨ä¸“å®¶, æ“…é•¿ä»å¤æ‚çš„è¡Œä¸šæ–‡æ¡£ä¸­æå–ä¿¡æ¯, å¹¶å°†å…¶è½¬åŒ–ä¸ºé«˜è´¨é‡ã€å¤šè§’åº¦çš„å¯¹è¯å¼æ•°æ®é›†, ç”¨äºå¤§æ¨¡å‹å¾®è°ƒã€‚ä½ çš„ä¸“é•¿é¢†åŸŸæ˜¯æ–°èƒ½æºå’Œå…‰ä¼è¡Œä¸šæ”¿ç­–åˆ†æã€‚

# èƒŒæ™¯

æˆ‘å°†æä¾›ä¸€ç¯‡å…³äºä¸­å›½æ–°èƒ½æºå…‰ä¼è¡Œä¸šæ”¿ç­–çš„ Markdown æ ¼å¼æ–°é—»æ–‡ç« ã€‚è¿™ç¯‡æ–‡ç« åŒ…å«äº†æ”¿ç­–ç»†èŠ‚ã€è¡Œä¸šå½±å“ã€ä¸“å®¶è§‚ç‚¹ç­‰ä¿¡æ¯ã€‚

# ä»»åŠ¡

ä½ çš„æ ¸å¿ƒä»»åŠ¡æ˜¯æ ¹æ®æˆ‘æä¾›çš„æ–‡ç« å†…å®¹, ç”Ÿæˆå°½å¯èƒ½å¤šçš„é«˜è´¨é‡çš„å¯¹è¯(Conversation)ã€‚æ¯ä¸€å¯¹å¯¹è¯éƒ½åº”åŒ…å«ä¸€ä¸ªâ€œç”¨æˆ·(user)â€çš„æé—®å’Œä¸€ä¸ªâ€œåŠ©æ‰‹(assistant)â€çš„å›ç­”ã€‚è¿™äº›å¯¹è¯éœ€è¦æ»¡è¶³ä»¥ä¸‹æ‰€æœ‰è¦æ±‚ã€‚

# å…³é”®è¦æ±‚

1.  **ç»å¯¹å¿ å®åŸæ–‡**:
    *   **æ‰€æœ‰**â€œåŠ©æ‰‹(assistant)â€çš„å›ç­”éƒ½**å¿…é¡»**å®Œå…¨åŸºäºæ‰€æä¾›æ–‡ç« çš„å†…å®¹, ä¸å¾—æé€ ã€æ­ªæ›²æˆ–å¼•å…¥æ–‡ç« ä¹‹å¤–çš„ä»»ä½•ä¿¡æ¯ã€‚
    *   å¦‚æœæ–‡ç« ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯æ¥å›ç­”æŸä¸ªé—®é¢˜, ä½ åº”è¯¥æ„æ€ä¸€ä¸ªç”¨æˆ·é—®é¢˜, ç„¶åè®©åŠ©æ‰‹å›ç­”â€œæ ¹æ®æ‰€æä¾›çš„æ–‡ç« , å…¶ä¸­å¹¶æœªæåŠå…³äº[é—®é¢˜ä¸»é¢˜]çš„å…·ä½“ä¿¡æ¯ã€‚â€

2.  **æ¨¡æ‹Ÿå¤šæ ·åŒ–ç”¨æˆ·è§’è‰² (Persona)**:
    *   æ„æ€â€œç”¨æˆ·(user)â€æé—®æ—¶, è¯·æ¨¡æ‹Ÿæ¥è‡ªä¸åŒèƒŒæ™¯çš„äººã€‚è¿™ä¼šè®©é—®é¢˜æ›´å¤šæ ·ã€æ›´æœ‰æ·±åº¦ã€‚è¯·è‡³å°‘è¦†ç›–ä»¥ä¸‹è§’è‰²ä¸­çš„ **4ç§**:
        *   **è¡Œä¸šæŠ•èµ„è€…**: å…³å¿ƒæŠ•èµ„å›æŠ¥ã€å¸‚åœºæœºä¼šã€é£é™©ç‚¹ã€‚
        *   **å…‰ä¼ä¼ä¸šä¸»/é«˜ç®¡**: å…³å¿ƒæ”¿ç­–å¯¹ä¼ä¸šè¿è¥ã€æˆæœ¬ã€ä¾›åº”é“¾ã€æŠ€æœ¯è·¯çº¿çš„å½±å“ã€‚
        *   **ä¸€çº¿å·¥ç¨‹å¸ˆ/æŠ€æœ¯äººå‘˜**: å…³å¿ƒæŠ€æœ¯æ ‡å‡†ã€å…·ä½“å‚æ•°ã€å®æ–½ç»†èŠ‚ã€‚
        *   **æ”¿ç­–ç ”ç©¶å‘˜/å­¦è€…**: å…³å¿ƒæ”¿ç­–è®¾è®¡çš„åˆè¡·ã€é•¿è¿œå½±å“ã€ä¸å…¶ä»–æ”¿ç­–çš„å…³è”ã€‚
        *   **æ™®é€šæ°‘ä¼—/æ½œåœ¨æ¶ˆè´¹è€…**: å…³å¿ƒæ”¿ç­–å¦‚ä½•å½±å“ç”µä»·ã€å¦‚ä½•ç”³è¯·è¡¥è´´ã€å¯¹è‡ªå·±ç”Ÿæ´»çš„å¥½å¤„ã€‚
        *   **æ–°é—»è®°è€…**: å¯»æ±‚å¯¹æ”¿ç­–æ ¸å¿ƒè¦ç‚¹çš„æ€»ç»“ã€å…³é”®æ•°æ®çš„æç‚¼æˆ–æœ‰å†²å‡»åŠ›çš„è§‚ç‚¹ã€‚

3.  **é—®é¢˜ç±»å‹å¤šæ ·åŒ–**:
    *   ä¸è¦åªé—®â€œæ˜¯ä»€ä¹ˆâ€ã€‚è¯·è®¾è®¡å¤šæ ·åŒ–çš„é—®é¢˜, æ¶µç›–ä¸åŒè®¤çŸ¥å±‚é¢ï¼š
        *   **ä¿¡æ¯æå–**: â€œæ”¿ç­–ä¸­æåˆ°çš„å…·ä½“è¡¥è´´é‡‘é¢æ˜¯å¤šå°‘ï¼Ÿâ€
        *   **æ ¸å¿ƒæ‘˜è¦**: â€œèƒ½å¦ç”¨ä¸‰å¥è¯æ€»ç»“ä¸€ä¸‹è¿™é¡¹æ–°æ”¿ç­–çš„æ ¸å¿ƒå†…å®¹ï¼Ÿâ€
        *   **å½±å“åˆ†æ**: â€œè¿™é¡¹æ”¿ç­–å¯¹åˆ†å¸ƒå¼å…‰ä¼çš„å‘å±•ä¼šæœ‰ä»€ä¹ˆå…·ä½“å½±å“ï¼Ÿâ€
        *   **åŸå› æ¢ç©¶**: â€œæ–‡ç« æœ‰æ²¡æœ‰æåˆ°ä¸ºä»€ä¹ˆé€‰æ‹©åœ¨è¿™ä¸ªæ—¶é—´ç‚¹å‡ºå°è¿™é¡¹æ”¿ç­–ï¼Ÿâ€
        *   **å¯¹æ¯”åˆ†æ**: â€œæ–°æ”¿ç­–å’Œæ—§æ”¿ç­–ç›¸æ¯”, ä¸»è¦æœ‰å“ªäº›ä¸åŒç‚¹ï¼Ÿâ€ (å¦‚æœæ–‡ç« æœ‰æåŠ)
        *   **æ“ä½œæŒ‡å—**: â€œå¦‚æœä¸€å®¶ä¼ä¸šæƒ³ç”³è¯·è¿™ä¸ªé¡¹ç›®, æ ¹æ®æ–‡ç« æè¿°, éœ€è¦æ»¡è¶³å“ªäº›æ¡ä»¶ï¼Ÿâ€

4.  **å›ç­”çš„ç»“æ„åŒ–ä¸ä¸“ä¸šæ€§**:
    *   â€œåŠ©æ‰‹(assistant)â€çš„å›ç­”åº”è¯¥æ¸…æ™°ã€å‡†ç¡®ã€ä¸“ä¸šã€‚
    *   å¦‚æœåŸæ–‡ä¿¡æ¯é€‚åˆ, å¯ä»¥ä½¿ç”¨åˆ—è¡¨ï¼ˆbullet pointsï¼‰æˆ–åˆ†ç‚¹é˜è¿°, ä½¿å›ç­”æ›´å…·æ¡ç†æ€§ã€‚

# è¾“å‡ºæ ¼å¼

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸‹é¢çš„ JSON æ ¼å¼è¾“å‡º:
\```json
{{"conversations": [ {{ "role": "user", "content": "é—®é¢˜1" }} , {{ "role": "assistant", "content": "å›ç­”1" }} ]}}
\```

è¿™æ˜¯ä¸€ä¸ªåŒ…å«å¤šä¸ªå¯¹è¯å¯¹è±¡çš„JSONå¯¹è±¡ã€‚`coversations`key åŒ…å«å¤šä¸ªå¯¹è¯, æ¯ä¸ªå¯¹è¯å¯¹è±¡éƒ½åŒ…å«ä¸€ä¸ª `user` å’Œä¸€ä¸ª `assistant` çš„è§’è‰²å’Œå†…å®¹ã€‚

åŸºäºä¸Šè¿°è¦æ±‚è¯·å¤„ç†ä»¥ä¸‹æ–‡ç« :

{markdown_text}
"""


def generate_conversation(markdown_text: str) -> List[str]:
    client = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
    prompt = build_prompt(markdown_text)
    try:
        completion = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
        )
        output = completion.choices.message.content
        if output is None:
            return []
        match = re.search(r"```json\s*([\s\S]*?)\s*```", output)
        if match:
            output = match.group(1).strip()
        parsed_output = json.loads(output)
        return parsed_output.get("conversations", [])
    except Exception as e:
        print(f"catch error: {e} ")
        return []
`````

# **3. ç”Ÿæˆ SFT è®­ç»ƒæ•°æ®ï¼šé€‚é…ä¸ä¼˜åŒ–**

ä¸Šä¸€æ­¥æˆ‘ä»¬å·²ç»ç”Ÿæˆäº†é€šç”¨çš„å¯¹è¯æ•°æ®ï¼Œä¸ºä»€ä¹ˆè¿˜éœ€è¦ä¸€ä¸ªä¸“é—¨çš„æ­¥éª¤æ¥â€œç”Ÿæˆè®­ç»ƒæ•°æ®â€å‘¢ï¼Ÿè¿™ä¸»è¦åŸºäºä¸¤ä¸ªé‡è¦åŸå› ï¼š

1.  **é€‚é…è®­ç»ƒæ¡†æ¶çš„æ•°æ®æ ¼å¼ï¼š**
    ä¸åŒçš„å¾®è°ƒæ¡†æ¶å¯¹æ•°æ®æ ¼å¼æœ‰ä¸åŒçš„è¦æ±‚ã€‚æˆ‘ä»¬æœ€åˆç”Ÿæˆçš„å¯¹è¯æ ¼å¼ï¼ˆ`[{'role': 'user', 'content': '...'}, {'role': 'assistant', 'content': '...'}]`ï¼‰æ˜¯ä¸€ç§é€šç”¨æ ‡å‡†ï¼Œä½†æœ¬æ–‡é‡‡ç”¨çš„ **AWS SageMaker JumpStart Llama 3.2 å¾®è°ƒè„šæœ¬**ï¼Œè¦æ±‚çš„æ˜¯ä¸€ç§æ›´ç®€æ´çš„**æŒ‡ä»¤æ ¼å¼**ï¼ˆInstruction-basedï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å°†å¯¹è¯æ•°æ®è½¬æ¢ä¸ºåŒ…å« `instruction` (é—®é¢˜) å’Œ `answer` (ç­”æ¡ˆ) é”®çš„ JSONL æ–‡ä»¶ï¼Œå¹¶è¾…ä»¥ä¸€ä¸ª `template.json` æ–‡ä»¶æ¥å‘Šè¯‰ SageMaker å¦‚ä½•å°†è¿™ä¸¤éƒ¨åˆ†ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„å¯¹è¯ã€‚

2.  **è§£å†³èƒ½åŠ›é€€åŒ–ï¼ˆæœ€ä½³å®è·µè¯´æ˜ï¼‰ï¼š**
    ä»…ä½¿ç”¨é«˜åº¦ä¸“ä¸šåŒ–çš„æ•°æ®è¿›è¡Œå¾®è°ƒï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹åœ¨å…¶ä»–é€šç”¨èƒ½åŠ›ä¸Šå‡ºç°â€œé—å¿˜â€æˆ–é€€åŒ–ã€‚ä¸šç•Œæ¨èçš„æœ€ä½³å®è·µæ˜¯å°†ä¸“ä¸šæ•°æ®ä¸é€šç”¨å¯¹è¯æ•°æ®æŒ‰ä¸€å®šæ¯”ä¾‹ï¼ˆå¦‚ 8:2ï¼‰æ··åˆè®­ç»ƒã€‚
    > **æ³¨æ„ï¼š** æœ¬æ•™ç¨‹ä¸ºäº†ç®€åŒ–æµç¨‹ï¼Œå¹¶æœªæ‰§è¡Œæ•°æ®èåˆæ­¥éª¤ï¼Œä½†åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¼ºçƒˆå»ºè®®å¼•å…¥æ­¤ç¯èŠ‚ä»¥ä¿è¯æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

- **ä»£ç å®ç°ï¼š**

```python
#
# å‡†å¤‡å¾®è°ƒæ•°æ®
#
import glob
import os
from pathlib import Path
from typing import Dict, Tuple
import pandas as pd


policy_info: Dict[str, Tuple[str, str]] = {}
df = pd.read_csv("output/policy_urls.csv")
for _, row in df.iterrows():
    policy_info[str(row["file_id"])] = (row["date"], row["title"])

all_csv_files = glob.glob(os.path.join("output/conversations", "*.csv"))
all_instructions = []
for csv_file in all_csv_files:
    file_id = Path(csv_file).stem
    _, title = policy_info[file_id]
    df = pd.read_csv(csv_file)
    instruction = {}
    for _, row in df.iterrows():
        role, content = row["role"], row["content"]
        if role == "user":
            content = f"å…³äºå…‰ä¼æ”¿ç­–<{title}>, ç”¨æˆ·æé—®:{content}"
        else:
            instruction["answer"] = content
            all_instructions.append(instruction)
            instruction = {}

pretty_df = pd.DataFrame(all_instructions)
pretty_df.to_json(
    "output/aws/train-instrucion-data/train.jsonl",
    orient="records",
    lines=True,
    force_ascii=False,
)
print(f"generate {len(pretty_df)} instructions")
```

æ•°æ®å‡†å¤‡å¥½åè¿˜éœ€è¦å‡†å¤‡æ¨¡æ¿ï¼Œæ¨¡æ¿å†…å®¹å¦‚ä¸‹ (`template.json`)ï¼š

```json
{
  "prompt": "å…³äºå…‰ä¼æ”¿ç­–<{context}>, æé—®: {question}",
  "completion": "{answer}"
}
```

# **4. æ‰§è¡Œå¾®è°ƒä»»åŠ¡ï¼šå¯åŠ¨ SageMaker JumpStart**

æ•°æ®å‡†å¤‡å°±ç»ªåï¼Œæˆ‘ä»¬å°±å¯ä»¥åˆ©ç”¨ SageMaker JumpStart æ¥æ‰§è¡Œå¾®è°ƒã€‚JumpStart æå¤§åœ°ç®€åŒ–äº†æµç¨‹ï¼Œå®ƒæä¾›äº†é¢„ç½®çš„è®­ç»ƒè„šæœ¬å’Œæ¨¡å‹é•œåƒï¼Œæˆ‘ä»¬åªéœ€é…ç½®å¥½ç¯å¢ƒå¹¶æäº¤æ•°æ®å³å¯ã€‚

- **æ ¸å¿ƒæ­¥éª¤ï¼š**

  1.  **é…ç½®ç¯å¢ƒï¼š** è®¾ç½®å¥½ AWS è§’è‰²ï¼ˆRoleï¼‰ã€åŒºåŸŸï¼ˆRegionï¼‰å’Œ S3 å­˜å‚¨æ¡¶ï¼ˆBucketï¼‰ã€‚
  2.  **ä¸Šä¼ æ•°æ®ï¼š** å°†ä¸Šä¸€æ­¥ç”Ÿæˆçš„ `train.jsonl` å’Œ `template.json` æ–‡ä»¶ä¸Šä¼ åˆ° S3 æŒ‡å®šä½ç½®ã€‚
  3.  **å®šä¹‰ Estimatorï¼š** åˆ›å»ºä¸€ä¸ª `JumpStartEstimator` å®ä¾‹ï¼Œåœ¨å…¶ä¸­æŒ‡å®šåŸºç¡€æ¨¡å‹ ID (`meta-textgeneration-llama-3-2-1b-instruct`)ã€è®­ç»ƒå®ä¾‹ç±»å‹ (`ml.g5.xlarge`) å’Œè¾“å‡ºè·¯å¾„ã€‚
  4.  **è®¾ç½®è¶…å‚æ•°ï¼š** é…ç½®è®­ç»ƒå‚æ•°ï¼Œå¦‚ `epoch`ï¼ˆè®­ç»ƒè½®æ•°ï¼‰ã€`max_input_length`ï¼ˆæœ€å¤§è¾“å…¥é•¿åº¦ï¼‰ç­‰ã€‚
  5.  **å¯åŠ¨è®­ç»ƒï¼š** è°ƒç”¨ `.fit()` æ–¹æ³•ï¼Œä¼ å…¥è®­ç»ƒæ•°æ®åœ¨ S3 çš„è·¯å¾„ï¼Œæ­£å¼å¼€å§‹å¾®è°ƒä»»åŠ¡ã€‚

- **ä»£ç å®ç°ï¼š**

```python
#
# ä¸Šä¼ æ•°æ®å¹¶å¾®è°ƒ
#
from time import strftime
import sagemaker
from sagemaker.s3 import S3Uploader
from sagemaker.jumpstart.estimator import JumpStartEstimator
import boto3

# é…ç½® SageMaker è§’è‰²å’Œ S3 è·¯å¾„
role = "arn:aws-cn:iam::xxxx:role/sandbox-SageMakerExecutorRole"
aws_region = "cn-northwest-1"
bucket = "sandbox-sagemaker"

boto_session = boto3.Session(region_name=aws_region)
sagemaker_session = sagemaker.Session(boto_session=boto_session)  # type: ignore


local_data_dir = "output/aws/train-instrucion-data" # æ•°æ®+æ¨¡æ¿
train_data_location = f"s3://{bucket}/llm-fine-tuning/train-data"
train_output_location = f"s3://{bucket}/llm-fine-tuning/output"
S3Uploader.upload(local_data_dir, train_data_location)
print(f"Training data: {train_data_location}")

# 1. find the model ID for the model of your choice in the Built-in Algorithms with pre-trained Model Table.
# model_id = "huggingface-llm-gemma-7b-instruct"
# model_version = "1.3.10"

model_id = "meta-textgeneration-llama-3-2-1b-instruct"
model_version = "1.2.5"

# 2. using the model ID, define your training job as a JumpStart estimator.
estimator = JumpStartEstimator(
    model_id=model_id,
    model_version=model_version,
    role=role,
    region=aws_region,
    sagemaker_session=sagemaker_session,
    environment={"accept_eula": "true"},
    instance_type="ml.g5.xlarge",
    output_path=train_output_location,
)
estimator.set_hyperparameters(
    chat_dataset=True, instruction_tuned=False, epoch="5", max_input_length="1024"
)

# 3. run estimator.fit() on your model, pointing to the training data to use for fine-tuning.
job_name = f"llama3-2-1b-finetune-{strftime('%Y-%m-%d-%H-%M-%S')}"
print(f"Starting training job: {job_name}")
estimator.fit(
    inputs={
        "training": train_data_location,
    },
)
```

# **5. æ¨¡å‹éƒ¨ç½²ï¼šä»è®­ç»ƒäº§ç‰©åˆ°åœ¨çº¿æœåŠ¡**

è®­ç»ƒä»»åŠ¡å®Œæˆåï¼ŒSageMaker ä¼šå°†å¾®è°ƒåçš„æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨ S3 ä¸­ã€‚ä¸‹ä¸€æ­¥æ˜¯å°†å…¶éƒ¨ç½²ä¸ºä¸€ä¸ªå¯ä»¥å®æ—¶è°ƒç”¨çš„ API ç«¯ç‚¹ï¼ˆEndpointï¼‰ã€‚

æœ¬æ–‡æä¾›äº†ä¸€ç§éå¸¸ç¨³å¥çš„éƒ¨ç½²æ–¹æ³•ï¼š**å…ˆå°†è®­ç»ƒä»»åŠ¡è¾“å‡ºçš„æ¨¡å‹æ–‡ä»¶ä¸‹è½½ã€é‡æ–°æ‰“åŒ…ï¼Œå†ä¸Šä¼ è‡³ S3ï¼Œæœ€åä½¿ç”¨è¯¥æ‰“åŒ…æ–‡ä»¶è¿›è¡Œéƒ¨ç½²**ã€‚è¿™ä¸ªè¿‡ç¨‹ç¡®ä¿äº†æ¨¡å‹åˆ¶å“çš„å®Œæ•´æ€§å’Œæ ¼å¼çš„è§„èŒƒæ€§ã€‚

> **å¤‡é€‰æ–¹æ¡ˆè¯´æ˜ï¼š** åœ¨è®¸å¤šæ ‡å‡†åœºæ™¯ä¸‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨è®­ç»ƒä»»åŠ¡è¾“å‡ºçš„ `S3ModelArtifacts` è·¯å¾„æ¥åˆ›å»º `JumpStartModel` å¹¶è¿›è¡Œéƒ¨ç½²ï¼Œæ— éœ€æ‰‹åŠ¨ä¸‹è½½å’Œé‡æ–°ä¸Šä¼ ã€‚æœ¬æ–‡æä¾›çš„è„šæœ¬ä¸ºæ‚¨å±•ç¤ºäº†ä¸€ä¸ªæ›´æ˜ç¡®ã€æ›´å¯æ§çš„æµç¨‹ï¼Œå°¤å…¶åœ¨éœ€è¦å¯¹æ¨¡å‹æ–‡ä»¶è¿›è¡Œæ£€æŸ¥æˆ–ä¿®æ”¹æ—¶éå¸¸æœ‰ç”¨ã€‚

- **ä»£ç å®ç°ï¼š**

```python
from urllib.parse import urlparse
import sagemaker
from sagemaker.jumpstart.model import JumpStartModel
import boto3
import subprocess
import os

# å’Œè®­ç»ƒæ—¶ä¿æŒä¸€è‡´
role = "arn:aws-cn:iam::xxxx:role/sandbox-SageMakerExecutorRole"
aws_region = "cn-northwest-1"
bucket = "sandbox-sagemaker"

boto_session = boto3.Session(region_name=aws_region)
sagemaker_session = sagemaker.Session(boto_session=boto_session)  # type: ignore
sagemaker_client = boto_session.client("sagemaker")
s3_client = boto_session.client("s3")

model_id = "meta-textgeneration-llama-3-2-1b-instruct"
model_version = "1.2.5"
training_job_name = "llama-3-2-1b-instruct-2025-07-04-04-58-52-402"  # MUST SETED

# ----- 1. ä½¿ç”¨ JumpStartModel ä»è®­ç»ƒä»»åŠ¡åˆ›å»ºæ¨¡å‹ -----
job_description = sagemaker_client.describe_training_job(
    TrainingJobName=training_job_name
)
model_data_s3_path = job_description["ModelArtifacts"]["S3ModelArtifacts"]
# image_uri = job_description["AlgorithmSpecification"]["TrainingImage"]
print(f"train job output: {model_data_s3_path}")

tmpdir = "./dist"
tmp_model_dir = os.path.join(tmpdir, "model_files")
local_packaged_model = os.path.join(tmpdir, "model.tar.gz")
os.makedirs(tmp_model_dir, exist_ok=True)
sync_command = ["aws", "s3", "sync", model_data_s3_path, tmp_model_dir]
subprocess.run(sync_command, capture_output=True, text=True)
print("sync model file done")
tar_command = ["tar", "-czvf", local_packaged_model, "-C", tmp_model_dir, "."]
subprocess.run(tar_command, capture_output=True, text=True)
print("tar model file done")
s3_url = urlparse(model_data_s3_path)
bucket_name = s3_url.netloc
target_s3_prefix = os.path.dirname(os.path.dirname(s3_url.path.lstrip("/")))
target_s3_key = f"{target_s3_prefix}/model.tar.gz"
s3_client.upload_file(local_packaged_model, bucket_name, target_s3_key)
final_model_data_path = f"s3://{bucket_name}/{target_s3_key}"
print(f"uploaded model tar.gz to {final_model_data_path}")
trained_model = JumpStartModel(
    model_id=model_id,
    model_version=model_version,
    model_data=final_model_data_path,
    role=role,
    sagemaker_session=sagemaker_session,
    instance_type="ml.g5.xlarge",
)

# ----- 2. éƒ¨ç½²æ¨¡å‹åˆ°ç«¯ç‚¹ -----
predictor = trained_model.deploy(
    instance_type="ml.g5.xlarge", endpoint_name=training_job_name, accept_eula=True
)

print(f"éƒ¨ç½²æˆåŠŸç«¯ç‚¹: {training_job_name}")
```

# **6. è°ƒç”¨æ¨¡å‹ï¼šä¸ä½ çš„ä¸“å±ä¸“å®¶å¯¹è¯**

æ¨¡å‹éƒ¨ç½²æˆåŠŸåï¼Œæˆ‘ä»¬å°±å¯ä»¥åƒè°ƒç”¨ä»»ä½• API ä¸€æ ·ä¸å®ƒäº¤äº’äº†ã€‚éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå³ä¾¿æ˜¯å¾®è°ƒåçš„æ¨¡å‹ï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦éµå¾ªå…¶åŸºç¡€æ¨¡å‹ï¼ˆLlama 3.2 Instructï¼‰çš„å¯¹è¯æ¨¡æ¿ï¼ˆChat Templateï¼‰æ¥æ„å»ºè¾“å…¥ï¼Œä»¥ç¡®ä¿æ¨¡å‹èƒ½æ­£ç¡®ç†è§£è§’è‰²å’Œå¯¹è¯è½®æ¬¡ã€‚

- **è°ƒç”¨æµç¨‹ï¼š**

  1.  **è·å–å¯¹è¯æ¨¡æ¿ï¼š** ä½¿ç”¨ `transformers` åº“çš„ `AutoTokenizer` ä¸‹è½½æˆ–åŠ è½½ Llama 3.2 çš„åˆ†è¯å™¨å’Œå¯¹è¯æ¨¡æ¿ã€‚
  2.  **æ„å»ºè¯·æ±‚ï¼š** å®šä¹‰ä¸€ä¸ªå¯¹è¯åˆ—è¡¨ï¼ˆåŒ…å« system prompt å’Œ user a messageï¼‰ï¼Œç„¶åä½¿ç”¨ `tokenizer.apply_chat_template()` æ–¹æ³•å°†å…¶è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿè¯†åˆ«çš„å­—ç¬¦ä¸²æ ¼å¼ã€‚
  3.  **å‘é€è¯·æ±‚ï¼š** ä½¿ç”¨ `boto3` å®¢æˆ·ç«¯ï¼Œå°†æ ¼å¼åŒ–åçš„è¾“å…¥å’Œæ¨ç†å‚æ•°ï¼ˆå¦‚ `max_new_tokens`, `temperature`ï¼‰å°è£…æˆ JSON `payload`ï¼Œè°ƒç”¨ SageMaker Runtime çš„ `invoke_endpoint` æ–¹æ³•ã€‚
  4.  **è§£æå“åº”ï¼š** è§£ç è¿”å›çš„ç»“æœï¼Œå³å¯çœ‹åˆ°å¾®è°ƒåæ¨¡å‹çš„å›ç­”ã€‚

- **ä»£ç å®ç°ï¼š**

```python
# download_assets.py
from transformers import AutoTokenizer
import os

# The model ID from Hugging Face Hub
model_id = "meta-llama/Llama-3.2-1B-Instruct"
# The local directory where you want to save the files
tokenizer_directory = "./output/tokenizers/llama-3.2-1b-instruct"

os.makedirs(tokenizer_directory, exist_ok=True)

print(f"Downloading tokenizer for '{model_id}' to '{tokenizer_directory}'...")

# Download and save the tokenizer files
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer.save_pretrained(tokenizer_directory)

print("Tokenizer downloaded and saved successfully.")

```

è°ƒç”¨æ¨¡å‹æµ‹è¯•è„šæœ¬å¦‚ä¸‹ï¼š

```python
import boto3
import json
from transformers import AutoTokenizer

# æ‚¨éœ€è¦éƒ¨ç½²æˆåŠŸçš„ç«¯ç‚¹çš„åç§°
endpoint_name = "llama-3-2-1b-instruct-2025-07-04-04-58-52-402"
aws_region = "cn-northwest-1"

boto_session = boto3.Session(region_name=aws_region)
runtime_client = boto_session.client(
    "sagemaker-runtime",
    region_name=aws_region,
)

# promt style 1: generate chat template from plain text
# user_prompt = (
#     "èƒ½å¦ç”¨ä¸‰å¥è¯æ€»ç»“ä¸€ä¸‹æ´›é˜³å¸‚æ–°å‡ºå°çš„å‡æ±¡é™ç¢³ååŒåˆ›æ–°è¯•ç‚¹å»ºè®¾å®æ–½æ–¹æ¡ˆçš„æ ¸å¿ƒå†…å®¹ï¼Ÿ"
# )
# formatted_prompt = (
#     f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
#     f"{user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
# )
# promt style 2: generate chat template from tokenizer
conversation = [
    {
        "role": "system",
        "content": "ä½ æ˜¯ä¸€ä¸ªæ–°èƒ½æºå…‰ä¼é¢†åŸŸçš„ä¸“å®¶, ä½ å¯ä»¥å¸®åŠ©å›ç­”å„ç±»å…‰ä¼é¢†åŸŸçš„é—®é¢˜, è¯·ç”¨ä¸­æ–‡å›ç­”é—®é¢˜ã€‚",
    },
    {
        "role": "user",
        "content": "ä»‹ç»ã€Šå…³äºè¿›ä¸€æ­¥åŠ å¿«æ­å·å¸‚å…‰ä¼å‘ç”µé¡¹ç›®å»ºè®¾çš„å®æ–½æ„è§ã€‹",
    },
]
# use the online/local model(meta-llama/Llama-3.2-1B-Instruct) tokenizer
tokenizer = AutoTokenizer.from_pretrained("./output/tokenizers/llama-3.2-1b-instruct")
formatted_prompt = tokenizer.apply_chat_template(
    conversation, tokenize=False, add_generation_prompt=True
)
print(f"{formatted_prompt}\n")
payload = {
    "inputs": formatted_prompt,
    "parameters": {
        "max_new_tokens": 1024,
        "top_p": 0.9,
        "temperature": 0.6,
        "repetition_penalty": 2.0,
        "stop": ["<|eot_id|>", "<|end_of_text|>"],
    },
}

encoded_payload = json.dumps(payload).encode("utf-8")
response = runtime_client.invoke_endpoint(
    EndpointName=endpoint_name,
    ContentType="application/json",
    Body=encoded_payload,
)
result_bytes = response["Body"].read()
result = result_bytes.decode("utf-8")
print(result)
```

# **7. æ€»ç»“ä¸å±•æœ›**

æœ¬æ–‡è¯¦ç»†åœ°èµ°è¿‡äº†ä»æ•°æ®é‡‡é›†åˆ°æ¨¡å‹éƒ¨ç½²çš„å…¨è¿‡ç¨‹ï¼ŒæˆåŠŸåœ°ä½¿ç”¨ AWS SageMaker å°†ä¸€ä¸ªé€šç”¨å¤§æ¨¡å‹å¾®è°ƒä¸ºäº†ä¸€ä¸ªå…‰ä¼æ”¿ç­–é¢†åŸŸçš„ä¸“å±æ¨¡å‹ã€‚è¿™ä¸ªç«¯åˆ°ç«¯çš„å®æˆ˜æ¡ˆä¾‹ä¸ºæ‚¨æä¾›äº†åœ¨è‡ªæœ‰æ•°æ®ä¸Šè¿›è¡Œæ¨¡å‹å¾®è°ƒçš„å®Œæ•´è“å›¾ã€‚

å½“ç„¶ï¼Œè¿™åªæ˜¯ä¸€ä¸ªèµ·ç‚¹ã€‚åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¦è·å¾—æœ€ä½³æ•ˆæœï¼Œè¿˜éœ€è¦åœ¨æ›´å¤šæ–¹é¢è¿›è¡Œæ¢ç´¢å’Œä¼˜åŒ–ï¼Œä¾‹å¦‚ï¼š

- **æ„å»ºéªŒè¯é›†ï¼š** åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ç›‘æ§æ¨¡å‹åœ¨æœªè§è¿‡æ•°æ®ä¸Šçš„è¡¨ç°ï¼Œä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚
- **è¶…å‚æ•°è°ƒä¼˜ï¼š** ç³»ç»Ÿæ€§åœ°å¯»æ‰¾æœ€ä½³çš„å­¦ä¹ ç‡ã€epoch ç­‰å‚æ•°ç»„åˆã€‚
- **æ•°æ®èåˆï¼š** å¦‚å‰æ–‡æ‰€è¿°ï¼Œèåˆé€šç”¨æ•°æ®ä»¥ä¿æŒæ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚

æˆ‘æœ¬äººä¹Ÿå¤„åœ¨ä¸æ–­å­¦ä¹ å’Œæ¢ç´¢çš„é˜¶æ®µï¼Œæ–‡ä¸­çš„å®è·µå¦‚æœ‰ä¸è¶³ä¹‹å¤„ï¼Œæ¬¢è¿å¤§å®¶äº¤æµæŒ‡æ­£ã€‚å¸Œæœ›è¿™ç¯‡æ•™ç¨‹èƒ½ä¸ºæ‚¨å¼€å¯å¤§æ¨¡å‹å¾®è°ƒä¹‹æ—…æä¾›æœ‰åŠ›çš„å¸®åŠ©ï¼
